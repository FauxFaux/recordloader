# simple loader for wikipedia's pages_full dump file
#
# wikipedia content can be downloaded from
# http://download.wikimedia.org/wikipedia/en/
#
# pages_full files are about 400 GB of XML, 
# and include every revision of every page. 
# we want each page element to become a document,
# and we have a fragment root defined for the revision element.
# 
# BE SURE TO CONFIGURE A FRAGMENT ROOT AS:
# namespace: http://www.mediawiki.org/xml/export-0.3/
# local-name: revision
#
# no INPUT_PATH: use stdin so we can avoid uncompressing the entire file.
# the command-line will look something like this:
#   7z e -so pages_full.xml.7z | recordloader.sh sample-wikipedia.properties
#
#CONNECTION_STRING=user:password@hostname:portnumber
ID_NAME=id
RECORD_NAME=page
RECORD_NAMESPACE=http://www.mediawiki.org/xml/export-0.3/
IGNORE_UNKNOWN=true
COLLECTIONS=wikipedia
URI_PREFIX=/wikipedia/
URI_SUFFIX=.xml
# change the namespace from the original to a new value:
# why? because we can...
DEFAULT_NAMESPACE=org.wikipedia.content
